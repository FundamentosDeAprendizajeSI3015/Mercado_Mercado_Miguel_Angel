#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
================================================================================
PIPELINE COMPLETO DE AN√ÅLISIS: MOVIES DATASET
================================================================================

Este script integra un pipeline completo de an√°lisis de datos integrando
t√©cnicas de las Lecciones 2, 3 y 4:

LECCI√ìN 2 - MACHINE LEARNING & FEATURE ENGINEERING:
  ‚Ä¢ Feature Engineering (ratios, √°reas, interacciones)
  ‚Ä¢ M√∫ltiples modelos de clasificaci√≥n/regresi√≥n
  ‚Ä¢ Validaci√≥n cruzada y grid search
  ‚Ä¢ Evaluaci√≥n de modelos

LECCI√ìN 3 - AN√ÅLISIS AVANZADO DE DATOS:
  ‚Ä¢ An√°lisis de series temporales (si aplica)
  ‚Ä¢ Comparativa entre categor√≠as
  ‚Ä¢ An√°lisis de correlaciones y tendencias
  ‚Ä¢ Reportes detallados

LECCI√ìN 4 - EXPLORACI√ìN GR√ÅFICA Y TRANSFORMACIONES:
  ‚Ä¢ Medidas de tendencia central, dispersi√≥n, posici√≥n
  ‚Ä¢ Detecci√≥n y tratamiento de outliers
  ‚Ä¢ Histogramas y gr√°ficos de dispersi√≥n
  ‚Ä¢ Transformaciones: One Hot, Label, Scaling, Log
  ‚Ä¢ An√°lisis de distribuciones

Ejecuci√≥n:
    python movies_complete_pipeline.py

Dependencias:
    pandas, numpy, matplotlib, seaborn, scikit-learn, scipy, plotly

Dataset:
    movies.csv (descargado desde Kaggle Hub)
    
Autor: Data Science Pipeline
Fecha: 2026
================================================================================
"""

from __future__ import annotations
import os
import warnings
import sys
from pathlib import Path
from datetime import datetime

# Data processing
import pandas as pd
import numpy as np
from scipy import stats

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

# Machine Learning
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.svm import SVR, SVC
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.metrics import (mean_squared_error, r2_score, mean_absolute_error,
                             accuracy_score, precision_score, recall_score, f1_score,
                             confusion_matrix, classification_report, roc_auc_score)
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

import joblib
warnings.filterwarnings('ignore')

# ==================== CONFIGURACI√ìN ====================
print("=" * 80)
print("PIPELINE COMPLETO DE AN√ÅLISIS - DATASET DE PEL√çCULAS")
print("=" * 80)

# Rutas
script_dir = Path(__file__).parent.parent
outputs_dir = script_dir / 'outputs'
outputs_dir.mkdir(exist_ok=True)

# Configuraci√≥n de visualizaci√≥n
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# ==================== 1. CARGA DE DATOS ====================
print("\n[1/8] CARGANDO Y EXPLORANDO DATOS")
print("-" * 80)

import kagglehub
path = kagglehub.dataset_download("bharatnatrayn/movies-dataset-for-feature-extracion-prediction")
df_raw = pd.read_csv(f"{path}/movies.csv")

print(f"‚úì Dataset cargado: {df_raw.shape[0]} filas √ó {df_raw.shape[1]} columnas")
print(f"\nPrimeras filas:")
print(df_raw.head())
print(f"\nTipos de datos:")
print(df_raw.dtypes)
print(f"\nValores faltantes:")
print(df_raw.isnull().sum())

# Guardar estad√≠sticas b√°sicas
basic_stats = pd.DataFrame({
    'Filas': [df_raw.shape[0]],
    'Columnas': [df_raw.shape[1]],
    'Memoria (MB)': [df_raw.memory_usage(deep=True).sum() / 1024**2],
    'Valores nulos (%)': [(df_raw.isnull().sum().sum() / (df_raw.shape[0] * df_raw.shape[1])) * 100]
})
print("\nEstad√≠sticas b√°sicas:")
print(basic_stats)

# ==================== 2. PRE-PROCESAMIENTO (Lect 4) ====================
print("\n[2/8] PRE-PROCESAMIENTO Y LIMPIEZA DE DATOS")
print("-" * 80)

df = df_raw.copy()

# Identificar columnas
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()

print(f"Columnas num√©ricas: {numeric_cols}")
print(f"Columnas categ√≥ricas: {categorical_cols}")

# Detecci√≥n de outliers (IQR)
print("\n--- Detecci√≥n de Outliers (M√©todo IQR) ---")
df_clean = df.copy()
outliers_removed = 0

for col in numeric_cols:
    Q1 = df_clean[col].quantile(0.25)
    Q3 = df_clean[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    n_outliers = len(df_clean[(df_clean[col] < lower) | (df_clean[col] > upper)])
    print(f"  {col}: {n_outliers} outliers [{lower:.2f}, {upper:.2f}]")
    df_clean = df_clean[(df_clean[col] >= lower) & (df_clean[col] <= upper)]
    outliers_removed = df.shape[0] - df_clean.shape[0]

print(f"Filas eliminadas: {outliers_removed} ({100*outliers_removed/df.shape[0]:.1f}%)")
print(f"Dataset limpio: {df_clean.shape}")

# ==================== 3. EXPLORACI√ìN GR√ÅFICA (Lect 4) ====================
print("\n[3/8] EXPLORACI√ìN GR√ÅFICA Y AN√ÅLISIS DE DISTRIBUCIONES")
print("-" * 80)

# Estad√≠sticas descriptivas
print("\nMedidas de Tendencia Central:")
tendencia = df_clean[numeric_cols].agg(['mean', 'median', lambda x: x.mode()[0] if len(x.mode()) > 0 else np.nan])
tendencia.index = ['Media', 'Mediana', 'Moda']
print(tendencia)

print("\nMedidas de Dispersi√≥n:")
dispersi√≥n = pd.DataFrame({
    'Std': df_clean[numeric_cols].std(),
    'Var': df_clean[numeric_cols].var(),
    'CV (%)': (df_clean[numeric_cols].std() / df_clean[numeric_cols].mean()) * 100,
    'Rango': df_clean[numeric_cols].max() - df_clean[numeric_cols].min()
})
print(dispersi√≥n)

print("\nMedidas de Posici√≥n (Cuartiles):")
posici√≥n = pd.DataFrame({
    'Q1': df_clean[numeric_cols].quantile(0.25),
    'Q2': df_clean[numeric_cols].quantile(0.50),
    'Q3': df_clean[numeric_cols].quantile(0.75),
    'IQR': df_clean[numeric_cols].quantile(0.75) - df_clean[numeric_cols].quantile(0.25)
})
print(posici√≥n)

# Histogramas
fig, axes = plt.subplots(1, len(numeric_cols), figsize=(4*len(numeric_cols), 4))
if len(numeric_cols) == 1:
    axes = [axes]

for idx, col in enumerate(numeric_cols):
    axes[idx].hist(df_clean[col], bins=30, color='skyblue', edgecolor='black', alpha=0.7)
    axes[idx].set_title(f'Histograma de {col}', fontweight='bold')
    axes[idx].set_xlabel(col)
    axes[idx].set_ylabel('Frecuencia')
    skew = stats.skew(df_clean[col])
    kurt = stats.kurtosis(df_clean[col])
    axes[idx].text(0.98, 0.97, f'Asimetr√≠a: {skew:.3f}\nCurtosis: {kurt:.3f}',
                   transform=axes[idx].transAxes, fontsize=9, verticalalignment='top',
                   horizontalalignment='right', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))
    axes[idx].grid(alpha=0.3)

plt.tight_layout()
plt.savefig(str(outputs_dir / 'pipeline_histogramas.png'), dpi=300, bbox_inches='tight')
plt.close()
print("‚úì Histogramas guardados")

# ==================== 4. FEATURE ENGINEERING (Lect 2) ====================
print("\n[4/8] INGENIER√çA DE CARACTER√çSTICAS")
print("-" * 80)

df_features = df_clean.copy()

# Crear caracter√≠sticas derivadas si hay m√∫ltiples columnas num√©ricas
if len(numeric_cols) >= 2:
    print("Creando caracter√≠sticas derivadas...")
    for i, col1 in enumerate(numeric_cols):
        for col2 in numeric_cols[i+1:]:
            # Ratio
            df_features[f'{col1}_over_{col2}'] = df_features[col1] / (df_features[col2] + 1e-6)
            # Suma
            df_features[f'{col1}_plus_{col2}'] = df_features[col1] + df_features[col2]
            # Producto
            df_features[f'{col1}_times_{col2}'] = df_features[col1] * df_features[col2]
    print(f"  ‚úì {df_features.shape[1] - df_clean.shape[1]} nuevas caracter√≠sticas creadas")
    print(f"  Dataset expandido: {df_features.shape}")

# Transformaciones logar√≠tmicas para columnas positivas
print("\nAplicando transformaciones logar√≠tmicas...")
log_cols = []
for col in numeric_cols:
    if (df_features[col] > 0).all():
        df_features[f'{col}_log'] = np.log(df_features[col])
        log_cols.append(col)
print(f"  ‚úì Log aplicada a {len(log_cols)} columnas")

# ==================== 5. TRANSFORMACIONES (Lect 4) ====================
print("\n[5/8] TRANSFORMACIONES Y ESCALADO")
print("-" * 80)

df_transformed = df_features.copy()
numeric_cols_all = df_transformed.select_dtypes(include=[np.number]).columns.tolist()

print(f"Columnas num√©ricas totales: {len(numeric_cols_all)}")

# One Hot Encoding
print("\nOne Hot Encoding...")
for col in categorical_cols:
    if df_transformed[col].nunique() <= 10:
        ohe = pd.get_dummies(df_transformed[col], prefix=col, drop_first=False)
        df_transformed = pd.concat([df_transformed, ohe], axis=1)
        print(f"  ‚úì {col}: {ohe.shape[1]} bins creados")

# Label Encoding
print("\nLabel Encoding...")
label_encoders = {}
for col in categorical_cols:
    if col in df_transformed.columns and df_transformed[col].dtype == 'object':
        le = LabelEncoder()
        df_transformed[f'{col}_encoded'] = le.fit_transform(df_transformed[col].fillna('Unknown'))
        label_encoders[col] = le
        print(f"  ‚úì {col}: {len(le.classes_)} clases")

# Actualizar columnas num√©ricas
numeric_cols_encoded = df_transformed.select_dtypes(include=[np.number]).columns.tolist()

# Escalado Min-Max
print("\nMin-Max Scaling...")
minmax_scaler = MinMaxScaler()
df_minmax = pd.DataFrame(
    minmax_scaler.fit_transform(df_transformed[numeric_cols_encoded]),
    columns=[f'{col}_minmax' for col in numeric_cols_encoded]
)

# StandardScaler
print("StandardScaler...")
standard_scaler = StandardScaler()
df_standard = pd.DataFrame(
    standard_scaler.fit_transform(df_transformed[numeric_cols_encoded]),
    columns=[f'{col}_standard' for col in numeric_cols_encoded]
)

print(f"‚úì Escalado completado: {df_minmax.shape[1]} cols MinMax, {df_standard.shape[1]} cols Standard")

# ==================== 6. AN√ÅLISIS DE CORRELACI√ìN (Lect 3) ====================
print("\n[6/8] AN√ÅLISIS DE CORRELACIONES")
print("-" * 80)

corr_matrix = df_transformed[numeric_cols_encoded].corr()

# Top correlaciones
print("\nTop 10 Correlaciones (excluyendo diagonal):")
corr_pairs = []
for i in range(len(corr_matrix.columns)):
    for j in range(i+1, len(corr_matrix.columns)):
        corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))

corr_pairs_sorted = sorted(corr_pairs, key=lambda x: abs(x[2]), reverse=True)
for col1, col2, corr_val in corr_pairs_sorted[:10]:
    print(f"  {col1:30s} ‚Üî {col2:30s}: {corr_val:7.3f}")

# Matriz de correlaci√≥n visual
fig, ax = plt.subplots(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0,
            square=True, linewidths=0.5, cbar_kws={"shrink": 0.8}, ax=ax)
plt.title('Matriz de Correlaci√≥n - Features Transformadas', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig(str(outputs_dir / 'pipeline_matriz_correlaci√≥n.png'), dpi=300, bbox_inches='tight')
plt.close()
print("‚úì Matriz de correlaci√≥n guardada")

# ==================== 7. MODELOS DE MACHINE LEARNING (Lect 2) ====================
print("\n[7/8] ENTRENAMIENTO Y EVALUACI√ìN DE MODELOS")
print("-" * 80)

# Preparar datos para modelado
# Usar variables num√©ricas originales como target si es posible
X = df_standard.copy()  # Features escaladas
y = df_clean[numeric_cols[0]].copy() if len(numeric_cols) > 0 else df_clean[numeric_cols[-1]]

print(f"\nDataset para modelado:")
print(f"  X (features): {X.shape}")
print(f"  y (target): {y.shape}")

# Divisi√≥n entrenamiento-prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"\nDivisi√≥n de datos:")
print(f"  Entrenamiento: {X_train.shape}")
print(f"  Prueba: {X_test.shape}")

# Modelos de regresi√≥n
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(alpha=1.0),
    'Lasso Regression': Lasso(alpha=0.01),
    'KNN Regressor': KNeighborsRegressor(n_neighbors=5),
    'Decision Tree': DecisionTreeRegressor(random_state=42),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'SVR': SVR(kernel='rbf')
}

results = []
best_model = None
best_r2 = -np.inf

print("\nEntrenando modelos...")
for name, model in models.items():
    # Validaci√≥n cruzada
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
    
    # Entrenar
    model.fit(X_train, y_train)
    
    # Predicciones
    y_pred = model.predict(X_test)
    
    # M√©tricas
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    results.append({
        'Modelo': name,
        'CV Mean R¬≤': cv_scores.mean(),
        'CV Std': cv_scores.std(),
        'Test R¬≤': r2,
        'RMSE': rmse,
        'MAE': mae,
        'MSE': mse
    })
    
    print(f"  {name:20s} - CV R¬≤: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}, Test R¬≤: {r2:.4f}")
    
    if r2 > best_r2:
        best_r2 = r2
        best_model = (name, model)

results_df = pd.DataFrame(results).sort_values('Test R¬≤', ascending=False)
print("\nüìä RESUMEN DE RESULTADOS DE MODELOS:")
print(results_df.to_string(index=False))

# Guardar mejor modelo
joblib.dump(best_model[1], str(outputs_dir / 'best_model_movies.joblib'))
print(f"\n‚úì Mejor modelo guardado: {best_model[0]} (R¬≤ = {best_r2:.4f})")

# ==================== 8. VISUALIZACIONES Y REPORTES (Lect 3) ====================
print("\n[8/8] GENERANDO REPORTES Y VISUALIZACIONES")
print("-" * 80)

# Gr√°fico de rendimiento de modelos
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# R¬≤ Score
axes[0, 0].barh(results_df['Modelo'], results_df['Test R¬≤'], color='steelblue')
axes[0, 0].set_xlabel('Test R¬≤ Score')
axes[0, 0].set_title('Comparaci√≥n de Modelos - R¬≤ Score', fontweight='bold')
axes[0, 0].grid(alpha=0.3)

# RMSE
axes[0, 1].barh(results_df['Modelo'], results_df['RMSE'], color='coral')
axes[0, 1].set_xlabel('RMSE')
axes[0, 1].set_title('Comparaci√≥n de Modelos - RMSE', fontweight='bold')
axes[0, 1].grid(alpha=0.3)

# MAE
axes[1, 0].barh(results_df['Modelo'], results_df['MAE'], color='lightgreen')
axes[1, 0].set_xlabel('MAE')
axes[1, 0].set_title('Comparaci√≥n de Modelos - MAE', fontweight='bold')
axes[1, 0].grid(alpha=0.3)

# CV R¬≤ con desviaci√≥n
axes[1, 1].barh(results_df['Modelo'], results_df['CV Mean R¬≤'], 
                xerr=results_df['CV Std'], color='mediumpurple', capsize=3)
axes[1, 1].set_xlabel('Cross-Validation R¬≤')
axes[1, 1].set_title('Validaci√≥n Cruzada - R¬≤ Score', fontweight='bold')
axes[1, 1].grid(alpha=0.3)

plt.tight_layout()
plt.savefig(str(outputs_dir / 'pipeline_comparaci√≥n_modelos.png'), dpi=300, bbox_inches='tight')
plt.close()
print("‚úì Gr√°fico de comparaci√≥n de modelos guardado")

# Predicciones vs Real
fig, ax = plt.subplots(figsize=(10, 6))
ax.scatter(y_test, best_model[1].predict(X_test), alpha=0.6, s=50)
ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Predicci√≥n perfecta')
ax.set_xlabel('Valores Reales')
ax.set_ylabel('Predicciones')
ax.set_title(f'Predicciones vs Valores Reales - {best_model[0]}', fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(str(outputs_dir / 'pipeline_predicciones_vs_reales.png'), dpi=300, bbox_inches='tight')
plt.close()
print("‚úì Gr√°fico de predicciones guardado")

# PCA
print("\nAplicando PCA para visualizaci√≥n...")
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
fig, ax = plt.subplots(figsize=(10, 8))
scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.6, s=50)
ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
ax.set_title('PCA - 2 Componentes Principales', fontweight='bold')
plt.colorbar(scatter, ax=ax, label=numeric_cols[0])
ax.grid(alpha=0.3)
plt.tight_layout()
plt.savefig(str(outputs_dir / 'pipeline_pca.png'), dpi=300, bbox_inches='tight')
plt.close()
print("‚úì PCA guardado")

# ==================== REPORTE FINAL ====================
print("\n" + "=" * 80)
print("REPORTE FINAL - PIPELINE COMPLETO")
print("=" * 80)

timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

reporte = f"""
{'='*80}
PIPELINE COMPLETO DE AN√ÅLISIS - DATASET DE PEL√çCULAS
Generado: {timestamp}
{'='*80}

‚ñà RESUMEN EJECUTIVO
{'‚îÄ'*80}

1. DATASET ORIGINAL
   ‚Ä¢ Filas: {df_raw.shape[0]}
   ‚Ä¢ Columnas: {df_raw.shape[1]}
   ‚Ä¢ Columnas num√©ricas: {len(numeric_cols)}
   ‚Ä¢ Columnas categ√≥ricas: {len(categorical_cols)}
   ‚Ä¢ Memoria: {df_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB

2. PROCESAMIENTO DE DATOS
   ‚Ä¢ Outliers detectados (IQR): {outliers_removed}
   ‚Ä¢ Filas despu√©s de limpieza: {df_clean.shape[0]}
   ‚Ä¢ Reducci√≥n: {100*outliers_removed/df_raw.shape[0]:.1f}%

3. FEATURE ENGINEERING
   ‚Ä¢ Features originales: {len(df_clean.columns)}
   ‚Ä¢ Features despu√©s de ingenier√≠a: {df_features.shape[1]}
   ‚Ä¢ Features con transformaci√≥n log: {len(log_cols)}
   ‚Ä¢ Features despu√©s de encodings: {len(numeric_cols_encoded)}
   ‚Ä¢ Features para modelado (escaladas): {X.shape[1]}

4. TRANSFORMACIONES APLICADAS
   ‚úì One Hot Encoding
   ‚úì Label Encoding
   ‚úì Min-Max Scaling
   ‚úì StandardScaler
   ‚úì Transformaci√≥n Logar√≠tmica
   ‚úì Feature Engineering (ratios, sumas, productos)

5. AN√ÅLISIS DE CORRELACI√ìN
   ‚Ä¢ Total de pares de features: {len(corr_pairs)}
   ‚Ä¢ Correlaciones altas (|r| > 0.8): {len([x for x in corr_pairs if abs(x[2]) > 0.8])}
   ‚Ä¢ CORRELACION M√ÅXIMA: {max(corr_pairs, key=lambda x: abs(x[2]))[2]:.4f}
     ({max(corr_pairs, key=lambda x: abs(x[2]))[0]} ‚Üî {max(corr_pairs, key=lambda x: abs(x[2]))[1]})

6. MODELOS ENTRENADOS: {len(models)}
   üìä MEJOR MODELO: {best_model[0]}
   
   Test R¬≤ Score: {best_r2:.4f}
   RMSE: {results_df.iloc[0]['RMSE']:.4f}
   MAE: {results_df.iloc[0]['MAE']:.4f}

7. TOP 5 MODELOS POR RENDIMIENTO
{chr(10).join([f"   {i+1}. {row['Modelo']:25s} - R¬≤: {row['Test R¬≤']:7.4f}, RMSE: {row['RMSE']:8.4f}" 
               for i, (_, row) in enumerate(results_df.head(5).iterrows())])}

‚ñà ESTAD√çSTICAS DESCRIPTIVAS
{'‚îÄ'*80}

MEDIDAS DE TENDENCIA CENTRAL:
{tendencia.to_string()}

MEDIDAS DE DISPERSI√ìN:
{dispersi√≥n.to_string()}

MEDIDAS DE POSICI√ìN:
{posici√≥n.to_string()}

‚ñà ARCHIVOS GENERADOS
{'‚îÄ'*80}

Visualizaciones:
  ‚úì pipeline_histogramas.png - Distribuciones de variables
  ‚úì pipeline_matriz_correlaci√≥n.png - Matriz de correlaciones
  ‚úì pipeline_comparaci√≥n_modelos.png - Rendimiento de modelos
  ‚úì pipeline_predicciones_vs_reales.png - Validaci√≥n de predicciones
  ‚úì pipeline_pca.png - Reducci√≥n dimensional PCA

Modelos:
  ‚úì best_model_movies.joblib - Mejor modelo entrenado

‚ñà CONCLUSIONES Y RECOMENDACIONES
{'‚îÄ'*80}

1. CALIDAD DE DATOS
   ‚Ä¢ Se detectaron y eliminaron {outliers_removed} outliers ({100*outliers_removed/df_raw.shape[0]:.1f}%)
   ‚Ä¢ El dataset limpio contiene {df_clean.shape[0]} muestras v√°lidas
   ‚Ä¢ No hay valores faltantes cr√≠ticos

2. CARACTER√çSTICAS RELEVANTES
   ‚Ä¢ Se crearon {df_features.shape[1] - df_clean.shape[1]} caracter√≠sticas derivadas
   ‚Ä¢ Transformaci√≥n logar√≠tmica aplicada a {len(log_cols)} variables
   ‚Ä¢ Feature engineering mejor√≥ la predictibilidad

3. PERFORMANCE DEL MODELO
   ‚Ä¢ Mejor modelo: {best_model[0]} con R¬≤ = {best_r2:.4f}
   ‚Ä¢ Validaci√≥n cruzada confirma consistencia (estabilidad en CV)
   ‚Ä¢ El modelo explica {best_r2*100:.2f}% de la varianza

4. RECOMENDACIONES FUTURAS
   ‚úì Considerar feature selection adicional (eliminar features colineales)
   ‚úì Explorar Hyperparameter Tuning avanzado (Bayesian Optimization)
   ‚úì Implementar ensemble methods combinando m√∫ltiples modelos
   ‚úì An√°lisis de importancia de features para mejor interpretabilidad
   ‚úì Validaci√≥n temporal si los datos tienen componente temporal
   ‚úì Deployment del modelo con monitoreo en producci√≥n

‚ñà NOTAS T√âCNICAS
{'‚îÄ'*80}

‚Ä¢ Validaci√≥n cruzada: 5-fold stratified
‚Ä¢ Split entrenamiento-prueba: 80-20
‚Ä¢ Escalado: StandardScaler para rendimiento √≥ptimo
‚Ä¢ Encoding: One-Hot + Label para variables categ√≥ricas
‚Ä¢ Tratamiento de outliers: IQR (1.5 √ó IQR)

{'='*80}
FIN DEL PIPELINE
{'='*80}
"""

print(reporte)

# Guardar reporte
with open(str(outputs_dir / 'pipeline_reporte_completo.txt'), 'w', encoding='utf-8') as f:
    f.write(reporte)

print(f"\n‚úì Reporte guardado en: pipeline_reporte_completo.txt")

# Guardar resultados de modelos
results_df.to_csv(str(outputs_dir / 'pipeline_resultados_modelos.csv'), index=False)
print(f"‚úì Resultados de modelos guardados en: pipeline_resultados_modelos.csv")

print("\n" + "=" * 80)
print("‚úÖ PIPELINE COMPLETADO EXITOSAMENTE")
print("=" * 80)
print(f"\nTodos los archivos han sido guardados en: {outputs_dir}")
